{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541f3166",
   "metadata": {},
   "source": [
    "# WGAN-GP 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e59b94",
   "metadata": {},
   "source": [
    "- 논문제목 : Improved Training of Wasserstein GANs\n",
    "- 학습 데이터셋: MINST (1x28x28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485051a",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916ad63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353135f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c92d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "img_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'wgangp_save/'\n",
    "dataset = 'MNIST'\n",
    "data_path = 'wgangp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4152687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.isdir(os.path.join(save_path, dataset)):\n",
    "    os.makedirs(os.path.join(save_path, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c15ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalize = lambda x: x*0.5+0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ddf36",
   "metadata": {},
   "source": [
    "## 학습 데이터셋 불러오기\n",
    "- 학습을 위해 MNIST 데이터셋을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e4ae05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=\"./wgangp\", train=True,download = True, transform=transforms_train)\n",
    "dataloader = DataLoader(train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be50bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x):\n",
    "    img = (np.array(x.detach().cpu(), dtype='float')).reshape(28,28)\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2e075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(train_dataset.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fc6da",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff5167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        \n",
    "        # 하나의 블록(block) 정의\n",
    "        def block(input_dim, output_dim, normalize=True):\n",
    "            layers = [nn.Linear(input_dim, output_dim)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        # 생성자 모델은 연속적인 여러 개의 블록을 가짐\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36debc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        output = self.model(flattened)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a963f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6256a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (11): Linear(in_features=1024, out_features=784, bias=True)\n",
       "  (12): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eca4cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b7984",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17944172",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penealty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    epsilon = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + ((1 - epsilon) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0),  requires_grad=False)\n",
    "    # Get gradient w.r.t interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs = d_interpolates,\n",
    "        inputs = interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) -1)**2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200b0b3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81a0d317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Batch [100/938] Discriminator loss: 1.1726 Generator loss: -0.2040\n",
      "Epoch [1/200] Batch [200/938] Discriminator loss: 0.9206 Generator loss: -0.1948\n",
      "Epoch [1/200] Batch [300/938] Discriminator loss: 1.1392 Generator loss: -0.2407\n",
      "Epoch [1/200] Batch [400/938] Discriminator loss: 0.9955 Generator loss: -0.3857\n",
      "Epoch [1/200] Batch [500/938] Discriminator loss: 0.6085 Generator loss: -0.2431\n",
      "Epoch [1/200] Batch [600/938] Discriminator loss: 1.0156 Generator loss: -0.4676\n",
      "Epoch [1/200] Batch [700/938] Discriminator loss: 0.6793 Generator loss: -0.3290\n",
      "Epoch [1/200] Batch [800/938] Discriminator loss: 0.2908 Generator loss: -0.1783\n",
      "Epoch [1/200] Batch [900/938] Discriminator loss: 0.5210 Generator loss: -0.2862\n",
      "Epoch [2/200] Batch [100/938] Discriminator loss: 0.4732 Generator loss: -0.3284\n",
      "Epoch [2/200] Batch [200/938] Discriminator loss: 0.0802 Generator loss: -0.1952\n",
      "Epoch [2/200] Batch [300/938] Discriminator loss: 0.0142 Generator loss: -0.3600\n",
      "Epoch [2/200] Batch [400/938] Discriminator loss: 0.1142 Generator loss: -0.2289\n",
      "Epoch [2/200] Batch [500/938] Discriminator loss: -0.0607 Generator loss: -0.2144\n",
      "Epoch [2/200] Batch [600/938] Discriminator loss: -0.1353 Generator loss: -0.4586\n",
      "Epoch [2/200] Batch [700/938] Discriminator loss: 0.1383 Generator loss: -0.3629\n",
      "Epoch [2/200] Batch [800/938] Discriminator loss: -0.1277 Generator loss: -0.1928\n",
      "Epoch [2/200] Batch [900/938] Discriminator loss: -0.1190 Generator loss: -0.3078\n",
      "Epoch [3/200] Batch [100/938] Discriminator loss: -0.1645 Generator loss: -0.2723\n",
      "Epoch [3/200] Batch [200/938] Discriminator loss: 2.0671 Generator loss: -0.6047\n",
      "Epoch [3/200] Batch [300/938] Discriminator loss: 0.0503 Generator loss: -0.3684\n",
      "Epoch [3/200] Batch [400/938] Discriminator loss: 0.3265 Generator loss: -0.4926\n",
      "Epoch [3/200] Batch [500/938] Discriminator loss: 9.9870 Generator loss: -0.9998\n",
      "Epoch [3/200] Batch [600/938] Discriminator loss: -0.2068 Generator loss: -0.3866\n",
      "Epoch [3/200] Batch [700/938] Discriminator loss: 9.9836 Generator loss: -0.9998\n",
      "Epoch [3/200] Batch [800/938] Discriminator loss: 0.1561 Generator loss: -0.4306\n",
      "Epoch [3/200] Batch [900/938] Discriminator loss: 9.9974 Generator loss: -0.0000\n",
      "Epoch [4/200] Batch [100/938] Discriminator loss: -0.0643 Generator loss: -0.3660\n",
      "Epoch [4/200] Batch [200/938] Discriminator loss: 8.9612 Generator loss: -0.9965\n",
      "Epoch [4/200] Batch [300/938] Discriminator loss: 9.9993 Generator loss: -0.0000\n",
      "Epoch [4/200] Batch [400/938] Discriminator loss: 9.9989 Generator loss: -0.0000\n",
      "Epoch [4/200] Batch [500/938] Discriminator loss: 9.9978 Generator loss: -0.0000\n",
      "Epoch [4/200] Batch [600/938] Discriminator loss: 5.4550 Generator loss: -0.0145\n",
      "Epoch [4/200] Batch [700/938] Discriminator loss: 9.9987 Generator loss: -1.0000\n",
      "Epoch [4/200] Batch [800/938] Discriminator loss: 9.9972 Generator loss: -0.9999\n",
      "Epoch [4/200] Batch [900/938] Discriminator loss: 0.6589 Generator loss: -0.1211\n",
      "Epoch [5/200] Batch [100/938] Discriminator loss: -0.0923 Generator loss: -0.3264\n",
      "Epoch [5/200] Batch [200/938] Discriminator loss: 9.9776 Generator loss: -0.9998\n",
      "Epoch [5/200] Batch [300/938] Discriminator loss: 0.4902 Generator loss: -0.9359\n",
      "Epoch [5/200] Batch [400/938] Discriminator loss: -0.1443 Generator loss: -0.3427\n",
      "Epoch [5/200] Batch [500/938] Discriminator loss: -0.0858 Generator loss: -0.3120\n",
      "Epoch [5/200] Batch [600/938] Discriminator loss: -0.2276 Generator loss: -0.3553\n",
      "Epoch [5/200] Batch [700/938] Discriminator loss: 1.4930 Generator loss: -0.1138\n",
      "Epoch [5/200] Batch [800/938] Discriminator loss: 0.2163 Generator loss: -0.6146\n",
      "Epoch [5/200] Batch [900/938] Discriminator loss: 7.1531 Generator loss: -0.3931\n",
      "Epoch [6/200] Batch [100/938] Discriminator loss: 0.0529 Generator loss: -0.6721\n",
      "Epoch [6/200] Batch [200/938] Discriminator loss: -0.1960 Generator loss: -0.3636\n",
      "Epoch [6/200] Batch [300/938] Discriminator loss: -0.0086 Generator loss: -0.4409\n",
      "Epoch [6/200] Batch [400/938] Discriminator loss: -0.2494 Generator loss: -0.2643\n",
      "Epoch [6/200] Batch [500/938] Discriminator loss: -0.2501 Generator loss: -0.3042\n",
      "Epoch [6/200] Batch [600/938] Discriminator loss: -0.2182 Generator loss: -0.3322\n",
      "Epoch [6/200] Batch [700/938] Discriminator loss: -0.1587 Generator loss: -0.3438\n",
      "Epoch [6/200] Batch [800/938] Discriminator loss: -0.2370 Generator loss: -0.3041\n",
      "Epoch [6/200] Batch [900/938] Discriminator loss: -0.2242 Generator loss: -0.3523\n",
      "Epoch [7/200] Batch [100/938] Discriminator loss: 0.0843 Generator loss: -0.3124\n",
      "Epoch [7/200] Batch [200/938] Discriminator loss: -0.1960 Generator loss: -0.2937\n",
      "Epoch [7/200] Batch [300/938] Discriminator loss: -0.2283 Generator loss: -0.4056\n",
      "Epoch [7/200] Batch [400/938] Discriminator loss: 0.2047 Generator loss: -0.1174\n",
      "Epoch [7/200] Batch [500/938] Discriminator loss: -0.2963 Generator loss: -0.3025\n",
      "Epoch [7/200] Batch [600/938] Discriminator loss: -0.1257 Generator loss: -0.4230\n",
      "Epoch [7/200] Batch [700/938] Discriminator loss: -0.1728 Generator loss: -0.3073\n",
      "Epoch [7/200] Batch [800/938] Discriminator loss: 9.8945 Generator loss: -0.9992\n",
      "Epoch [7/200] Batch [900/938] Discriminator loss: -0.2365 Generator loss: -0.2391\n",
      "Epoch [8/200] Batch [100/938] Discriminator loss: -0.1719 Generator loss: -0.4211\n",
      "Epoch [8/200] Batch [200/938] Discriminator loss: -0.0911 Generator loss: -0.2947\n",
      "Epoch [8/200] Batch [300/938] Discriminator loss: 0.2895 Generator loss: -0.0412\n",
      "Epoch [8/200] Batch [400/938] Discriminator loss: -0.1794 Generator loss: -0.4172\n",
      "Epoch [8/200] Batch [500/938] Discriminator loss: -0.0763 Generator loss: -0.3541\n",
      "Epoch [8/200] Batch [600/938] Discriminator loss: -0.1416 Generator loss: -0.3285\n",
      "Epoch [8/200] Batch [700/938] Discriminator loss: -0.2502 Generator loss: -0.2541\n",
      "Epoch [8/200] Batch [800/938] Discriminator loss: -0.2237 Generator loss: -0.4625\n",
      "Epoch [8/200] Batch [900/938] Discriminator loss: -0.2224 Generator loss: -0.2661\n",
      "Epoch [9/200] Batch [100/938] Discriminator loss: -0.2528 Generator loss: -0.3135\n",
      "Epoch [9/200] Batch [200/938] Discriminator loss: -0.3057 Generator loss: -0.3194\n",
      "Epoch [9/200] Batch [300/938] Discriminator loss: 1.5996 Generator loss: -0.5301\n",
      "Epoch [9/200] Batch [400/938] Discriminator loss: -0.1522 Generator loss: -0.2400\n",
      "Epoch [9/200] Batch [500/938] Discriminator loss: -0.1616 Generator loss: -0.3877\n",
      "Epoch [9/200] Batch [600/938] Discriminator loss: -0.2044 Generator loss: -0.4539\n",
      "Epoch [9/200] Batch [700/938] Discriminator loss: 0.1489 Generator loss: -0.3695\n",
      "Epoch [9/200] Batch [800/938] Discriminator loss: -0.1871 Generator loss: -0.3079\n",
      "Epoch [9/200] Batch [900/938] Discriminator loss: -0.2006 Generator loss: -0.3407\n",
      "Epoch [10/200] Batch [100/938] Discriminator loss: -0.2388 Generator loss: -0.2947\n",
      "Epoch [10/200] Batch [200/938] Discriminator loss: -0.3039 Generator loss: -0.2632\n",
      "Epoch [10/200] Batch [300/938] Discriminator loss: -0.3250 Generator loss: -0.3644\n",
      "Epoch [10/200] Batch [400/938] Discriminator loss: 0.2107 Generator loss: -0.3034\n",
      "Epoch [10/200] Batch [500/938] Discriminator loss: -0.3066 Generator loss: -0.3022\n",
      "Epoch [10/200] Batch [600/938] Discriminator loss: -0.2118 Generator loss: -0.3406\n",
      "Epoch [10/200] Batch [700/938] Discriminator loss: -0.0920 Generator loss: -0.2332\n",
      "Epoch [10/200] Batch [800/938] Discriminator loss: -0.2985 Generator loss: -0.3512\n",
      "Epoch [10/200] Batch [900/938] Discriminator loss: -0.2663 Generator loss: -0.2090\n",
      "Epoch [11/200] Batch [100/938] Discriminator loss: -0.2742 Generator loss: -0.2847\n",
      "Epoch [11/200] Batch [200/938] Discriminator loss: -0.1387 Generator loss: -0.4619\n",
      "Epoch [11/200] Batch [300/938] Discriminator loss: 1.7624 Generator loss: -0.7545\n",
      "Epoch [11/200] Batch [400/938] Discriminator loss: -0.2634 Generator loss: -0.3223\n",
      "Epoch [11/200] Batch [500/938] Discriminator loss: 0.1325 Generator loss: -0.3314\n",
      "Epoch [11/200] Batch [600/938] Discriminator loss: -0.2689 Generator loss: -0.4027\n",
      "Epoch [11/200] Batch [700/938] Discriminator loss: -0.3518 Generator loss: -0.3545\n",
      "Epoch [11/200] Batch [800/938] Discriminator loss: -0.2667 Generator loss: -0.2623\n",
      "Epoch [11/200] Batch [900/938] Discriminator loss: -0.3022 Generator loss: -0.3589\n",
      "Epoch [12/200] Batch [100/938] Discriminator loss: -0.2936 Generator loss: -0.2723\n",
      "Epoch [12/200] Batch [200/938] Discriminator loss: 1.5544 Generator loss: -0.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200] Batch [300/938] Discriminator loss: -0.2798 Generator loss: -0.3241\n",
      "Epoch [12/200] Batch [400/938] Discriminator loss: -0.0637 Generator loss: -0.5279\n",
      "Epoch [12/200] Batch [500/938] Discriminator loss: -0.2449 Generator loss: -0.2846\n",
      "Epoch [12/200] Batch [600/938] Discriminator loss: 0.0667 Generator loss: -0.5396\n",
      "Epoch [12/200] Batch [700/938] Discriminator loss: -0.2691 Generator loss: -0.3626\n",
      "Epoch [12/200] Batch [800/938] Discriminator loss: -0.1798 Generator loss: -0.4454\n",
      "Epoch [12/200] Batch [900/938] Discriminator loss: -0.0651 Generator loss: -0.3540\n",
      "Epoch [13/200] Batch [100/938] Discriminator loss: -0.2677 Generator loss: -0.3197\n",
      "Epoch [13/200] Batch [200/938] Discriminator loss: -0.2041 Generator loss: -0.3337\n",
      "Epoch [13/200] Batch [300/938] Discriminator loss: -0.2438 Generator loss: -0.3536\n",
      "Epoch [13/200] Batch [400/938] Discriminator loss: -0.2091 Generator loss: -0.3235\n",
      "Epoch [13/200] Batch [500/938] Discriminator loss: 0.2607 Generator loss: -0.5174\n",
      "Epoch [13/200] Batch [600/938] Discriminator loss: 0.2516 Generator loss: -0.4615\n",
      "Epoch [13/200] Batch [700/938] Discriminator loss: 0.4983 Generator loss: -0.5327\n",
      "Epoch [13/200] Batch [800/938] Discriminator loss: 1.0272 Generator loss: -0.2673\n",
      "Epoch [13/200] Batch [900/938] Discriminator loss: -0.0779 Generator loss: -0.3793\n",
      "Epoch [14/200] Batch [100/938] Discriminator loss: -0.2256 Generator loss: -0.3568\n",
      "Epoch [14/200] Batch [200/938] Discriminator loss: -0.2103 Generator loss: -0.2753\n",
      "Epoch [14/200] Batch [300/938] Discriminator loss: -0.1151 Generator loss: -0.2148\n",
      "Epoch [14/200] Batch [400/938] Discriminator loss: -0.2758 Generator loss: -0.3032\n",
      "Epoch [14/200] Batch [500/938] Discriminator loss: -0.0907 Generator loss: -0.4302\n",
      "Epoch [14/200] Batch [600/938] Discriminator loss: 6.2630 Generator loss: -0.4471\n",
      "Epoch [14/200] Batch [700/938] Discriminator loss: -0.0735 Generator loss: -0.4497\n",
      "Epoch [14/200] Batch [800/938] Discriminator loss: -0.0749 Generator loss: -0.4225\n",
      "Epoch [14/200] Batch [900/938] Discriminator loss: -0.2098 Generator loss: -0.2676\n",
      "Epoch [15/200] Batch [100/938] Discriminator loss: -0.1973 Generator loss: -0.3441\n",
      "Epoch [15/200] Batch [200/938] Discriminator loss: -0.3003 Generator loss: -0.3530\n",
      "Epoch [15/200] Batch [300/938] Discriminator loss: -0.2790 Generator loss: -0.2838\n",
      "Epoch [15/200] Batch [400/938] Discriminator loss: -0.0580 Generator loss: -0.3481\n",
      "Epoch [15/200] Batch [500/938] Discriminator loss: -0.2554 Generator loss: -0.3209\n",
      "Epoch [15/200] Batch [600/938] Discriminator loss: -0.2115 Generator loss: -0.2339\n",
      "Epoch [15/200] Batch [700/938] Discriminator loss: -0.2024 Generator loss: -0.3508\n",
      "Epoch [15/200] Batch [800/938] Discriminator loss: -0.0925 Generator loss: -0.3012\n",
      "Epoch [15/200] Batch [900/938] Discriminator loss: -0.2722 Generator loss: -0.3272\n",
      "Epoch [16/200] Batch [100/938] Discriminator loss: -0.1961 Generator loss: -0.4136\n",
      "Epoch [16/200] Batch [200/938] Discriminator loss: -0.2462 Generator loss: -0.3377\n",
      "Epoch [16/200] Batch [300/938] Discriminator loss: -0.2123 Generator loss: -0.3512\n",
      "Epoch [16/200] Batch [400/938] Discriminator loss: -0.1904 Generator loss: -0.3794\n",
      "Epoch [16/200] Batch [500/938] Discriminator loss: -0.1102 Generator loss: -0.3598\n",
      "Epoch [16/200] Batch [600/938] Discriminator loss: -0.1944 Generator loss: -0.3222\n",
      "Epoch [16/200] Batch [700/938] Discriminator loss: -0.2833 Generator loss: -0.3722\n",
      "Epoch [16/200] Batch [800/938] Discriminator loss: -0.1180 Generator loss: -0.4426\n",
      "Epoch [16/200] Batch [900/938] Discriminator loss: -0.3011 Generator loss: -0.3080\n",
      "Epoch [17/200] Batch [100/938] Discriminator loss: -0.2044 Generator loss: -0.3666\n",
      "Epoch [17/200] Batch [200/938] Discriminator loss: -0.1771 Generator loss: -0.3560\n",
      "Epoch [17/200] Batch [300/938] Discriminator loss: -0.2274 Generator loss: -0.3473\n",
      "Epoch [17/200] Batch [400/938] Discriminator loss: 6.6024 Generator loss: -0.0858\n",
      "Epoch [17/200] Batch [500/938] Discriminator loss: -0.2655 Generator loss: -0.3125\n",
      "Epoch [17/200] Batch [600/938] Discriminator loss: -0.2160 Generator loss: -0.3330\n",
      "Epoch [17/200] Batch [700/938] Discriminator loss: -0.2708 Generator loss: -0.2602\n",
      "Epoch [17/200] Batch [800/938] Discriminator loss: -0.3076 Generator loss: -0.3157\n",
      "Epoch [17/200] Batch [900/938] Discriminator loss: -0.2093 Generator loss: -0.2654\n",
      "Epoch [18/200] Batch [100/938] Discriminator loss: -0.2916 Generator loss: -0.3416\n",
      "Epoch [18/200] Batch [200/938] Discriminator loss: -0.2014 Generator loss: -0.4063\n",
      "Epoch [18/200] Batch [300/938] Discriminator loss: -0.2388 Generator loss: -0.3144\n",
      "Epoch [18/200] Batch [400/938] Discriminator loss: -0.1858 Generator loss: -0.3478\n",
      "Epoch [18/200] Batch [500/938] Discriminator loss: -0.2845 Generator loss: -0.3343\n",
      "Epoch [18/200] Batch [600/938] Discriminator loss: -0.0980 Generator loss: -0.5348\n",
      "Epoch [18/200] Batch [700/938] Discriminator loss: -0.2509 Generator loss: -0.3125\n",
      "Epoch [18/200] Batch [800/938] Discriminator loss: -0.3141 Generator loss: -0.3343\n",
      "Epoch [18/200] Batch [900/938] Discriminator loss: -0.2153 Generator loss: -0.3619\n",
      "Epoch [19/200] Batch [100/938] Discriminator loss: 0.0948 Generator loss: -0.3790\n",
      "Epoch [19/200] Batch [200/938] Discriminator loss: -0.2265 Generator loss: -0.3516\n",
      "Epoch [19/200] Batch [300/938] Discriminator loss: -0.2793 Generator loss: -0.2875\n",
      "Epoch [19/200] Batch [400/938] Discriminator loss: -0.2249 Generator loss: -0.3318\n",
      "Epoch [19/200] Batch [500/938] Discriminator loss: -0.1766 Generator loss: -0.3196\n",
      "Epoch [19/200] Batch [600/938] Discriminator loss: 5.9011 Generator loss: -0.2090\n",
      "Epoch [19/200] Batch [700/938] Discriminator loss: -0.1858 Generator loss: -0.3941\n",
      "Epoch [19/200] Batch [800/938] Discriminator loss: -0.0367 Generator loss: -0.3305\n",
      "Epoch [19/200] Batch [900/938] Discriminator loss: -0.2090 Generator loss: -0.3338\n",
      "Epoch [20/200] Batch [100/938] Discriminator loss: -0.2852 Generator loss: -0.3532\n",
      "Epoch [20/200] Batch [200/938] Discriminator loss: -0.1030 Generator loss: -0.4573\n",
      "Epoch [20/200] Batch [300/938] Discriminator loss: -0.2362 Generator loss: -0.2695\n",
      "Epoch [20/200] Batch [400/938] Discriminator loss: -0.0109 Generator loss: -0.3940\n",
      "Epoch [20/200] Batch [500/938] Discriminator loss: -0.1985 Generator loss: -0.4260\n",
      "Epoch [20/200] Batch [600/938] Discriminator loss: -0.1921 Generator loss: -0.3258\n",
      "Epoch [20/200] Batch [700/938] Discriminator loss: -0.1785 Generator loss: -0.3138\n",
      "Epoch [20/200] Batch [800/938] Discriminator loss: -0.2505 Generator loss: -0.3150\n",
      "Epoch [20/200] Batch [900/938] Discriminator loss: -0.2512 Generator loss: -0.3715\n",
      "Epoch [21/200] Batch [100/938] Discriminator loss: -0.2117 Generator loss: -0.3079\n",
      "Epoch [21/200] Batch [200/938] Discriminator loss: -0.1393 Generator loss: -0.4465\n",
      "Epoch [21/200] Batch [300/938] Discriminator loss: -0.2655 Generator loss: -0.2838\n",
      "Epoch [21/200] Batch [400/938] Discriminator loss: -0.1033 Generator loss: -0.2992\n",
      "Epoch [21/200] Batch [500/938] Discriminator loss: -0.2804 Generator loss: -0.3623\n",
      "Epoch [21/200] Batch [600/938] Discriminator loss: -0.2695 Generator loss: -0.3259\n",
      "Epoch [21/200] Batch [700/938] Discriminator loss: -0.2846 Generator loss: -0.2915\n",
      "Epoch [21/200] Batch [800/938] Discriminator loss: -0.2259 Generator loss: -0.2754\n",
      "Epoch [21/200] Batch [900/938] Discriminator loss: -0.2136 Generator loss: -0.3378\n",
      "Epoch [22/200] Batch [100/938] Discriminator loss: -0.1653 Generator loss: -0.4388\n",
      "Epoch [22/200] Batch [200/938] Discriminator loss: -0.2205 Generator loss: -0.3665\n",
      "Epoch [22/200] Batch [300/938] Discriminator loss: -0.2477 Generator loss: -0.3753\n",
      "Epoch [22/200] Batch [400/938] Discriminator loss: -0.2948 Generator loss: -0.3155\n",
      "Epoch [22/200] Batch [500/938] Discriminator loss: -0.1117 Generator loss: -0.4655\n",
      "Epoch [22/200] Batch [600/938] Discriminator loss: -0.2459 Generator loss: -0.3230\n",
      "Epoch [22/200] Batch [700/938] Discriminator loss: -0.2131 Generator loss: -0.3490\n",
      "Epoch [22/200] Batch [800/938] Discriminator loss: -0.1975 Generator loss: -0.3112\n",
      "Epoch [22/200] Batch [900/938] Discriminator loss: -0.2800 Generator loss: -0.3768\n",
      "Epoch [23/200] Batch [100/938] Discriminator loss: -0.2947 Generator loss: -0.3305\n",
      "Epoch [23/200] Batch [200/938] Discriminator loss: -0.2529 Generator loss: -0.5649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/200] Batch [300/938] Discriminator loss: -0.2715 Generator loss: -0.3272\n",
      "Epoch [23/200] Batch [400/938] Discriminator loss: -0.1949 Generator loss: -0.3845\n",
      "Epoch [23/200] Batch [500/938] Discriminator loss: -0.2193 Generator loss: -0.2433\n",
      "Epoch [23/200] Batch [600/938] Discriminator loss: -0.2149 Generator loss: -0.3562\n",
      "Epoch [23/200] Batch [700/938] Discriminator loss: -0.1838 Generator loss: -0.3348\n",
      "Epoch [23/200] Batch [800/938] Discriminator loss: 0.1915 Generator loss: -0.0895\n",
      "Epoch [23/200] Batch [900/938] Discriminator loss: -0.2221 Generator loss: -0.5677\n",
      "Epoch [24/200] Batch [100/938] Discriminator loss: -0.2789 Generator loss: -0.4234\n",
      "Epoch [24/200] Batch [200/938] Discriminator loss: -0.1423 Generator loss: -0.5443\n",
      "Epoch [24/200] Batch [300/938] Discriminator loss: -0.2042 Generator loss: -0.3088\n",
      "Epoch [24/200] Batch [400/938] Discriminator loss: -0.2125 Generator loss: -0.3476\n",
      "Epoch [24/200] Batch [500/938] Discriminator loss: 9.9984 Generator loss: -0.0000\n",
      "Epoch [24/200] Batch [600/938] Discriminator loss: -0.0792 Generator loss: -0.4261\n",
      "Epoch [24/200] Batch [700/938] Discriminator loss: -0.2341 Generator loss: -0.3665\n",
      "Epoch [24/200] Batch [800/938] Discriminator loss: -0.1199 Generator loss: -0.3153\n",
      "Epoch [24/200] Batch [900/938] Discriminator loss: -0.2402 Generator loss: -0.3891\n",
      "Epoch [25/200] Batch [100/938] Discriminator loss: -0.2302 Generator loss: -0.3783\n",
      "Epoch [25/200] Batch [200/938] Discriminator loss: -0.1391 Generator loss: -0.3580\n",
      "Epoch [25/200] Batch [300/938] Discriminator loss: -0.2378 Generator loss: -0.3136\n",
      "Epoch [25/200] Batch [400/938] Discriminator loss: 0.0416 Generator loss: -0.3356\n",
      "Epoch [25/200] Batch [500/938] Discriminator loss: -0.2019 Generator loss: -0.3917\n",
      "Epoch [25/200] Batch [600/938] Discriminator loss: -0.2210 Generator loss: -0.3988\n",
      "Epoch [25/200] Batch [700/938] Discriminator loss: -0.2169 Generator loss: -0.2697\n",
      "Epoch [25/200] Batch [800/938] Discriminator loss: 0.0805 Generator loss: -0.0477\n",
      "Epoch [25/200] Batch [900/938] Discriminator loss: -0.2687 Generator loss: -0.2965\n",
      "Epoch [26/200] Batch [100/938] Discriminator loss: -0.0845 Generator loss: -0.3403\n",
      "Epoch [26/200] Batch [200/938] Discriminator loss: -0.2386 Generator loss: -0.3012\n",
      "Epoch [26/200] Batch [300/938] Discriminator loss: -0.2693 Generator loss: -0.3256\n",
      "Epoch [26/200] Batch [400/938] Discriminator loss: -0.0392 Generator loss: -0.3979\n",
      "Epoch [26/200] Batch [500/938] Discriminator loss: -0.2168 Generator loss: -0.3255\n",
      "Epoch [26/200] Batch [600/938] Discriminator loss: -0.2099 Generator loss: -0.3699\n",
      "Epoch [26/200] Batch [700/938] Discriminator loss: 0.0110 Generator loss: -0.3964\n",
      "Epoch [26/200] Batch [800/938] Discriminator loss: -0.1796 Generator loss: -0.3530\n",
      "Epoch [26/200] Batch [900/938] Discriminator loss: -0.2712 Generator loss: -0.4212\n",
      "Epoch [27/200] Batch [100/938] Discriminator loss: -0.2393 Generator loss: -0.3265\n",
      "Epoch [27/200] Batch [200/938] Discriminator loss: -0.2792 Generator loss: -0.3240\n",
      "Epoch [27/200] Batch [300/938] Discriminator loss: -0.1860 Generator loss: -0.3244\n",
      "Epoch [27/200] Batch [400/938] Discriminator loss: -0.1799 Generator loss: -0.3708\n",
      "Epoch [27/200] Batch [500/938] Discriminator loss: -0.0807 Generator loss: -0.3989\n",
      "Epoch [27/200] Batch [600/938] Discriminator loss: -0.2154 Generator loss: -0.3448\n",
      "Epoch [27/200] Batch [700/938] Discriminator loss: -0.2585 Generator loss: -0.3446\n",
      "Epoch [27/200] Batch [800/938] Discriminator loss: -0.1921 Generator loss: -0.3459\n",
      "Epoch [27/200] Batch [900/938] Discriminator loss: -0.2289 Generator loss: -0.2909\n",
      "Epoch [28/200] Batch [100/938] Discriminator loss: -0.2130 Generator loss: -0.3315\n",
      "Epoch [28/200] Batch [200/938] Discriminator loss: -0.2117 Generator loss: -0.2966\n",
      "Epoch [28/200] Batch [300/938] Discriminator loss: -0.2333 Generator loss: -0.3461\n",
      "Epoch [28/200] Batch [400/938] Discriminator loss: -0.0631 Generator loss: -0.4022\n",
      "Epoch [28/200] Batch [500/938] Discriminator loss: -0.0644 Generator loss: -0.4394\n",
      "Epoch [28/200] Batch [600/938] Discriminator loss: -0.2046 Generator loss: -0.3788\n",
      "Epoch [28/200] Batch [700/938] Discriminator loss: -0.2198 Generator loss: -0.2956\n",
      "Epoch [28/200] Batch [800/938] Discriminator loss: -0.1627 Generator loss: -0.4211\n",
      "Epoch [28/200] Batch [900/938] Discriminator loss: -0.1422 Generator loss: -0.3684\n",
      "Epoch [29/200] Batch [100/938] Discriminator loss: -0.2442 Generator loss: -0.3465\n",
      "Epoch [29/200] Batch [200/938] Discriminator loss: -0.1333 Generator loss: -0.5727\n",
      "Epoch [29/200] Batch [300/938] Discriminator loss: -0.0514 Generator loss: -0.4362\n",
      "Epoch [29/200] Batch [400/938] Discriminator loss: -0.1429 Generator loss: -0.4166\n",
      "Epoch [29/200] Batch [500/938] Discriminator loss: -0.2105 Generator loss: -0.3330\n",
      "Epoch [29/200] Batch [600/938] Discriminator loss: -0.1982 Generator loss: -0.4460\n",
      "Epoch [29/200] Batch [700/938] Discriminator loss: -0.2212 Generator loss: -0.5186\n",
      "Epoch [29/200] Batch [800/938] Discriminator loss: -0.2442 Generator loss: -0.3962\n",
      "Epoch [29/200] Batch [900/938] Discriminator loss: -0.1908 Generator loss: -0.3810\n",
      "Epoch [30/200] Batch [100/938] Discriminator loss: -0.2268 Generator loss: -0.2939\n",
      "Epoch [30/200] Batch [200/938] Discriminator loss: -0.2334 Generator loss: -0.4512\n",
      "Epoch [30/200] Batch [300/938] Discriminator loss: -0.0645 Generator loss: -0.3321\n",
      "Epoch [30/200] Batch [400/938] Discriminator loss: -0.2324 Generator loss: -0.4534\n",
      "Epoch [30/200] Batch [500/938] Discriminator loss: -0.1198 Generator loss: -0.4834\n",
      "Epoch [30/200] Batch [600/938] Discriminator loss: 1.7482 Generator loss: -0.7108\n",
      "Epoch [30/200] Batch [700/938] Discriminator loss: 0.3201 Generator loss: -0.2364\n",
      "Epoch [30/200] Batch [800/938] Discriminator loss: -0.0221 Generator loss: -0.4974\n",
      "Epoch [30/200] Batch [900/938] Discriminator loss: -0.2497 Generator loss: -0.3772\n",
      "Epoch [31/200] Batch [100/938] Discriminator loss: -0.1532 Generator loss: -0.4340\n",
      "Epoch [31/200] Batch [200/938] Discriminator loss: -0.1950 Generator loss: -0.3883\n",
      "Epoch [31/200] Batch [300/938] Discriminator loss: -0.2028 Generator loss: -0.3940\n",
      "Epoch [31/200] Batch [400/938] Discriminator loss: 9.9981 Generator loss: -0.0000\n",
      "Epoch [31/200] Batch [500/938] Discriminator loss: -0.1895 Generator loss: -0.4086\n",
      "Epoch [31/200] Batch [600/938] Discriminator loss: -0.2157 Generator loss: -0.4272\n",
      "Epoch [31/200] Batch [700/938] Discriminator loss: -0.0826 Generator loss: -0.4072\n",
      "Epoch [31/200] Batch [800/938] Discriminator loss: -0.2066 Generator loss: -0.3516\n",
      "Epoch [31/200] Batch [900/938] Discriminator loss: -0.1514 Generator loss: -0.3715\n",
      "Epoch [32/200] Batch [100/938] Discriminator loss: 0.0424 Generator loss: -0.4618\n",
      "Epoch [32/200] Batch [200/938] Discriminator loss: -0.2041 Generator loss: -0.3642\n",
      "Epoch [32/200] Batch [300/938] Discriminator loss: -0.2044 Generator loss: -0.3606\n",
      "Epoch [32/200] Batch [400/938] Discriminator loss: -0.2339 Generator loss: -0.3942\n",
      "Epoch [32/200] Batch [500/938] Discriminator loss: -0.1968 Generator loss: -0.4952\n",
      "Epoch [32/200] Batch [600/938] Discriminator loss: -0.0576 Generator loss: -0.4586\n",
      "Epoch [32/200] Batch [700/938] Discriminator loss: -0.2175 Generator loss: -0.3336\n",
      "Epoch [32/200] Batch [800/938] Discriminator loss: -0.2055 Generator loss: -0.4158\n",
      "Epoch [32/200] Batch [900/938] Discriminator loss: -0.1939 Generator loss: -0.4475\n",
      "Epoch [33/200] Batch [100/938] Discriminator loss: -0.1764 Generator loss: -0.3432\n",
      "Epoch [33/200] Batch [200/938] Discriminator loss: -0.1294 Generator loss: -0.3037\n",
      "Epoch [33/200] Batch [300/938] Discriminator loss: 0.3973 Generator loss: -0.5976\n",
      "Epoch [33/200] Batch [400/938] Discriminator loss: -0.2066 Generator loss: -0.3577\n",
      "Epoch [33/200] Batch [500/938] Discriminator loss: -0.2066 Generator loss: -0.4009\n",
      "Epoch [33/200] Batch [600/938] Discriminator loss: -0.1791 Generator loss: -0.4385\n",
      "Epoch [33/200] Batch [700/938] Discriminator loss: -0.2448 Generator loss: -0.3030\n",
      "Epoch [33/200] Batch [800/938] Discriminator loss: -0.2177 Generator loss: -0.3249\n",
      "Epoch [33/200] Batch [900/938] Discriminator loss: -0.2017 Generator loss: -0.3649\n",
      "Epoch [34/200] Batch [100/938] Discriminator loss: -0.1392 Generator loss: -0.3995\n",
      "Epoch [34/200] Batch [200/938] Discriminator loss: -0.0712 Generator loss: -0.5806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/200] Batch [300/938] Discriminator loss: -0.1283 Generator loss: -0.3752\n",
      "Epoch [34/200] Batch [400/938] Discriminator loss: -0.2242 Generator loss: -0.3907\n",
      "Epoch [34/200] Batch [500/938] Discriminator loss: -0.1760 Generator loss: -0.3153\n",
      "Epoch [34/200] Batch [600/938] Discriminator loss: 0.0599 Generator loss: -0.4509\n",
      "Epoch [34/200] Batch [700/938] Discriminator loss: -0.1165 Generator loss: -0.3903\n",
      "Epoch [34/200] Batch [800/938] Discriminator loss: -0.1837 Generator loss: -0.3336\n",
      "Epoch [34/200] Batch [900/938] Discriminator loss: -0.2037 Generator loss: -0.3717\n",
      "Epoch [35/200] Batch [100/938] Discriminator loss: -0.1964 Generator loss: -0.3598\n",
      "Epoch [35/200] Batch [200/938] Discriminator loss: -0.1125 Generator loss: -0.4449\n",
      "Epoch [35/200] Batch [300/938] Discriminator loss: 8.6906 Generator loss: -0.9947\n",
      "Epoch [35/200] Batch [400/938] Discriminator loss: -0.1612 Generator loss: -0.3839\n",
      "Epoch [35/200] Batch [500/938] Discriminator loss: -0.2002 Generator loss: -0.4787\n",
      "Epoch [35/200] Batch [600/938] Discriminator loss: -0.1803 Generator loss: -0.3883\n",
      "Epoch [35/200] Batch [700/938] Discriminator loss: -0.1563 Generator loss: -0.4527\n",
      "Epoch [35/200] Batch [800/938] Discriminator loss: -0.2456 Generator loss: -0.3933\n",
      "Epoch [35/200] Batch [900/938] Discriminator loss: 1.8577 Generator loss: -0.1700\n",
      "Epoch [36/200] Batch [100/938] Discriminator loss: -0.2349 Generator loss: -0.3919\n",
      "Epoch [36/200] Batch [200/938] Discriminator loss: -0.2000 Generator loss: -0.4436\n",
      "Epoch [36/200] Batch [300/938] Discriminator loss: -0.1846 Generator loss: -0.3787\n",
      "Epoch [36/200] Batch [400/938] Discriminator loss: -0.1622 Generator loss: -0.3924\n",
      "Epoch [36/200] Batch [500/938] Discriminator loss: -0.1884 Generator loss: -0.3725\n",
      "Epoch [36/200] Batch [600/938] Discriminator loss: -0.2331 Generator loss: -0.3225\n",
      "Epoch [36/200] Batch [700/938] Discriminator loss: -0.0127 Generator loss: -0.4041\n",
      "Epoch [36/200] Batch [800/938] Discriminator loss: -0.1209 Generator loss: -0.4681\n",
      "Epoch [36/200] Batch [900/938] Discriminator loss: -0.2011 Generator loss: -0.3807\n",
      "Epoch [37/200] Batch [100/938] Discriminator loss: -0.2010 Generator loss: -0.3770\n",
      "Epoch [37/200] Batch [200/938] Discriminator loss: -0.1768 Generator loss: -0.4441\n",
      "Epoch [37/200] Batch [300/938] Discriminator loss: -0.1867 Generator loss: -0.3393\n",
      "Epoch [37/200] Batch [400/938] Discriminator loss: -0.2474 Generator loss: -0.3244\n",
      "Epoch [37/200] Batch [500/938] Discriminator loss: -0.1754 Generator loss: -0.4453\n",
      "Epoch [37/200] Batch [600/938] Discriminator loss: -0.1927 Generator loss: -0.4187\n",
      "Epoch [37/200] Batch [700/938] Discriminator loss: 0.3659 Generator loss: -0.5563\n",
      "Epoch [37/200] Batch [800/938] Discriminator loss: -0.1659 Generator loss: -0.3924\n",
      "Epoch [37/200] Batch [900/938] Discriminator loss: -0.1726 Generator loss: -0.3157\n",
      "Epoch [38/200] Batch [100/938] Discriminator loss: -0.2027 Generator loss: -0.4172\n",
      "Epoch [38/200] Batch [200/938] Discriminator loss: -0.1950 Generator loss: -0.4359\n",
      "Epoch [38/200] Batch [300/938] Discriminator loss: -0.2029 Generator loss: -0.3734\n",
      "Epoch [38/200] Batch [400/938] Discriminator loss: -0.2129 Generator loss: -0.3960\n",
      "Epoch [38/200] Batch [500/938] Discriminator loss: -0.2189 Generator loss: -0.4134\n",
      "Epoch [38/200] Batch [600/938] Discriminator loss: -0.2157 Generator loss: -0.3460\n",
      "Epoch [38/200] Batch [700/938] Discriminator loss: -0.1315 Generator loss: -0.4041\n",
      "Epoch [38/200] Batch [800/938] Discriminator loss: -0.1918 Generator loss: -0.4139\n",
      "Epoch [38/200] Batch [900/938] Discriminator loss: -0.1782 Generator loss: -0.4419\n",
      "Epoch [39/200] Batch [100/938] Discriminator loss: -0.1966 Generator loss: -0.4463\n",
      "Epoch [39/200] Batch [200/938] Discriminator loss: -0.2022 Generator loss: -0.2747\n",
      "Epoch [39/200] Batch [300/938] Discriminator loss: -0.1477 Generator loss: -0.4104\n",
      "Epoch [39/200] Batch [400/938] Discriminator loss: -0.1851 Generator loss: -0.3650\n",
      "Epoch [39/200] Batch [500/938] Discriminator loss: -0.1501 Generator loss: -0.4170\n",
      "Epoch [39/200] Batch [600/938] Discriminator loss: -0.2222 Generator loss: -0.3385\n",
      "Epoch [39/200] Batch [700/938] Discriminator loss: -0.2146 Generator loss: -0.4053\n",
      "Epoch [39/200] Batch [800/938] Discriminator loss: -0.1954 Generator loss: -0.4296\n",
      "Epoch [39/200] Batch [900/938] Discriminator loss: -0.2292 Generator loss: -0.3692\n",
      "Epoch [40/200] Batch [100/938] Discriminator loss: -0.1870 Generator loss: -0.3166\n",
      "Epoch [40/200] Batch [200/938] Discriminator loss: -0.2208 Generator loss: -0.3959\n",
      "Epoch [40/200] Batch [300/938] Discriminator loss: -0.1698 Generator loss: -0.4164\n",
      "Epoch [40/200] Batch [400/938] Discriminator loss: -0.1844 Generator loss: -0.3735\n",
      "Epoch [40/200] Batch [500/938] Discriminator loss: -0.1803 Generator loss: -0.4017\n",
      "Epoch [40/200] Batch [600/938] Discriminator loss: -0.1702 Generator loss: -0.3762\n",
      "Epoch [40/200] Batch [700/938] Discriminator loss: -0.1031 Generator loss: -0.4277\n",
      "Epoch [40/200] Batch [800/938] Discriminator loss: -0.1917 Generator loss: -0.3912\n",
      "Epoch [40/200] Batch [900/938] Discriminator loss: -0.1998 Generator loss: -0.3999\n",
      "Epoch [41/200] Batch [100/938] Discriminator loss: -0.1877 Generator loss: -0.4179\n",
      "Epoch [41/200] Batch [200/938] Discriminator loss: -0.1604 Generator loss: -0.4042\n",
      "Epoch [41/200] Batch [300/938] Discriminator loss: -0.1316 Generator loss: -0.4655\n",
      "Epoch [41/200] Batch [400/938] Discriminator loss: -0.1920 Generator loss: -0.4053\n",
      "Epoch [41/200] Batch [500/938] Discriminator loss: -0.1856 Generator loss: -0.3864\n",
      "Epoch [41/200] Batch [600/938] Discriminator loss: -0.1933 Generator loss: -0.3278\n",
      "Epoch [41/200] Batch [700/938] Discriminator loss: -0.1865 Generator loss: -0.3686\n",
      "Epoch [41/200] Batch [800/938] Discriminator loss: -0.1904 Generator loss: -0.2905\n",
      "Epoch [41/200] Batch [900/938] Discriminator loss: -0.1402 Generator loss: -0.4099\n",
      "Epoch [42/200] Batch [100/938] Discriminator loss: -0.1976 Generator loss: -0.4722\n",
      "Epoch [42/200] Batch [200/938] Discriminator loss: -0.1738 Generator loss: -0.3857\n",
      "Epoch [42/200] Batch [300/938] Discriminator loss: -0.2686 Generator loss: -0.4162\n",
      "Epoch [42/200] Batch [400/938] Discriminator loss: -0.0948 Generator loss: -0.3088\n",
      "Epoch [42/200] Batch [500/938] Discriminator loss: -0.0368 Generator loss: -0.3586\n",
      "Epoch [42/200] Batch [600/938] Discriminator loss: -0.1401 Generator loss: -0.4335\n",
      "Epoch [42/200] Batch [700/938] Discriminator loss: -0.2204 Generator loss: -0.3489\n",
      "Epoch [42/200] Batch [800/938] Discriminator loss: -0.1558 Generator loss: -0.3669\n",
      "Epoch [42/200] Batch [900/938] Discriminator loss: -0.1907 Generator loss: -0.4132\n",
      "Epoch [43/200] Batch [100/938] Discriminator loss: -0.2084 Generator loss: -0.4114\n",
      "Epoch [43/200] Batch [200/938] Discriminator loss: -0.1225 Generator loss: -0.4814\n",
      "Epoch [43/200] Batch [300/938] Discriminator loss: -0.1065 Generator loss: -0.3679\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4a1f3605d059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Configure input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "n_critic = 5\n",
    "log_interval = 100\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                Train Discriminator                  #\n",
    "        # ====================================================#\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # samplenoise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        # Real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                   Train Generator                   #\n",
    "        # ====================================================#\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "            \n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "        if (i+1) % log_interval == 0:\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            d_loss_list.append(d_loss.item())\n",
    "            print('Epoch [{}/{}] Batch [{}/{}] Discriminator loss: {:.4f} Generator loss: {:.4f}'.format(\n",
    "                epoch+1, n_epochs, i+1, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
